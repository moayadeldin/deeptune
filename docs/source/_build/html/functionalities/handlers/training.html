

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Using DeepTune for Training &mdash; DeepTune 1.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=fc837d61"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Using DeepTune for Evaluation" href="evaluation.html" />
    <link rel="prev" title="Handling Datasets" href="split.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            DeepTune
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../guides/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/demos.html">Demos</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/firstrun.html">Your First <strong>DeepTune</strong> Run</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Functionalities</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="split.html">Handling Datasets</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Using <strong>DeepTune</strong> for Training</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#images">Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="#text">Text</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tabular">Tabular</a></li>
<li class="toctree-l2"><a class="reference internal" href="#time-series">Time Series</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tabpfn-support">TabPFN Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training-output">Training Output</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="evaluation.html">Using <strong>DeepTune</strong> for Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="embedding.html">Using <strong>DeepTune</strong> for Knowledge Extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="dfanalyze.html">[EXTRA] Integration with df-analyze</a></li>
<li class="toctree-l1"><a class="reference internal" href="autoencoders.html">Using Autoencoders in <strong>DeepTune</strong></a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">DeepTune</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Using <strong>DeepTune</strong> for Training</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/functionalities/handlers/training.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="using-deeptune-for-training">
<h1>Using <strong>DeepTune</strong> for Training<a class="headerlink" href="#using-deeptune-for-training" title="Link to this heading"></a></h1>
<section id="images">
<h2>Images<a class="headerlink" href="#images" title="Link to this heading"></a></h2>
<p>The following is the generic CLI structure of running <strong>DeepTune</strong> on images dataset stored in Parquet file as bytes format for training:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>trainers.vision.train<span class="w"> </span><span class="se">\</span>
--train_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--val_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--model_version<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--batch_size<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--num_classes<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--num_epochs<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--learning_rate<span class="w"> </span>&lt;float&gt;<span class="w"> </span><span class="se">\</span>
--added_layers<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--embed_size<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--out<span class="w"> </span>&lt;str&gt;
<span class="go">--mode &lt;cls_or_reg&gt; \</span>
<span class="go">[--fixed-seed] \</span>
<span class="go">[--use-peft] \</span>
<span class="go">[--freeze-backbone]</span>
</pre></div>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--train_df</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Path to your training dataset (must be a parquet file).</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--val_df</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Path to your validation dataset (must be a parquet file).</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--&lt;model&gt;_version</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>The model to use along with its respective architecture version.
You may refer to the <em>Supported Models</em> table for available options.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--batch_size</span> <span class="pre">&lt;int&gt;</span></code></p></td>
<td><p>Number of samples per batch.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--num_classes</span> <span class="pre">&lt;int&gt;</span></code></p></td>
<td><p>Number of classes in your dataset.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--num_epochs</span> <span class="pre">&lt;int&gt;</span></code></p></td>
<td><p>Number of training epochs.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--learning_rate</span> <span class="pre">&lt;float&gt;</span></code></p></td>
<td><p>Learning rate used for optimization.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--added_layers</span> <span class="pre">&lt;int&gt;</span></code></p></td>
<td><p>Number of layers added on top of the model for transfer learning (either with or without using PeFT).
Only values <strong>1</strong> or <strong>2</strong> are supported currently.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--embed_size</span> <span class="pre">&lt;int&gt;</span></code></p></td>
<td><p>Size of the intermediate embedding layer (applicable when using two added layers).</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--out</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Path to the directory where you want to save the results.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--fixed-seed</span></code></p></td>
<td><p><em>(Flag)</em> Ensures that a fixed random seed is set for reproducibility.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--mode</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Task mode: either <code class="docutils literal notranslate"><span class="pre">cls</span></code> for classification or <code class="docutils literal notranslate"><span class="pre">reg</span></code> for regression.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--use-peft</span></code></p></td>
<td><p><em>(Flag)</em> Enables <strong>Parameter-Efficient Fine-Tuning (PeFT)</strong>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--freeze-backbone</span></code></p></td>
<td><p><em>(Flag)</em> Determines whether to train only the added layers or update all model parameters during training.</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For using PeFT just add the <cite>–use-peft</cite> switch to the previous command.</p>
</div>
<p>For example, suppose that we want to train our model with ResNet18, and apply transfer learning to update the whole model’s weights, and an embedding layer of size 1000. Hence, we run the command as follows:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>trainers.vision.train<span class="w"> </span>--train_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span>--val_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span>--model_version<span class="w"> </span>resnet18<span class="w"> </span>--batch_size<span class="w"> </span><span class="m">4</span><span class="w"> </span>--num_classes<span class="w"> </span><span class="m">2</span><span class="w"> </span>--num_epochs<span class="w"> </span><span class="m">10</span><span class="w"> </span>--learning_rate<span class="w"> </span><span class="m">0</span>.0001<span class="w"> </span>--added_layers<span class="w"> </span><span class="m">2</span><span class="w"> </span>--embed_size<span class="w"> </span><span class="m">1000</span><span class="w"> </span>--out<span class="w"> </span>&lt;str&gt;<span class="w"> </span>--mode<span class="w"> </span>cls<span class="w"> </span>--fixed-seed
</pre></div>
</div>
<p>If everything is set correctly, you should expect an output in the same format:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>&gt; os.environ[&#39;PYTHONHASHSEED&#39;] set to 42.
&gt; np.random.seed(42) set.
&gt; torch.manual_seed(42) set.
&gt; torch.cuda.manual_seed(42) set.
&gt; torch.cuda.manual_seed_all(42) set.
&gt; torch.backends.cudnn.benchmark set to False.
&gt; torch.backends.cudnn.deterministic set to True.
&gt; Dataset is loaded!
&gt; Data splits have been saved and overwritten if they existed.
&gt; The Trainer class is loaded successfully.

&gt; 4%|████    | 459/855 [00:17&lt;01:07,  5.89it/s, loss=0.43]
</pre></div>
</div>
</section>
<section id="text">
<h2>Text<a class="headerlink" href="#text" title="Link to this heading"></a></h2>
<p>Since <strong>DeepTune</strong> currently supports only two models for text classification, the way they are called in the CLI differs from that of image models. Apart from this, the CLI structure remains largely the same:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>GPT-2 model does not support PeFT right now in <strong>DeepTune</strong>.</p>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>trainers.nlp.<span class="o">[</span>train_multilingualbert/train_gpt2<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--train_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--val_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--batch_size<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_classes<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_epochs<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>&lt;float&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--added_layers<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--embed_size<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--fixed-seed<span class="o">]</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="o">[</span>--freeze_backbone<span class="o">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There is no need to specify the <code class="docutils literal notranslate"><span class="pre">--added_layers</span></code> and <code class="docutils literal notranslate"><span class="pre">--embed_size</span></code> switches when using GPT-2
as they are already statically fixed due to design constraints.</p>
</div>
</section>
<section id="tabular">
<h2>Tabular<a class="headerlink" href="#tabular" title="Link to this heading"></a></h2>
<p>Currently, <strong>DeepTune</strong> offers only support for GANDALF (Gated Adaptive Network for Deep Automated Learning of Features) model to provide predictions on your own tabular data. You can read more about GANDALF through the paper <a class="reference external" href="https://arxiv.org/abs/2207.08548">here</a>.</p>
<p>The generic CLI workflow for applying GANDALF in <strong>DeepTune</strong> requires specifying certain columns before training can begin, which is mainly determining the continuous columns in your dataset and the categorical columns as an input.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>GANDALF implementation does not support transfer learning in the way we commonly applied to images or text above. Instead, it follows the standard training scheme, starting from scratch.</p>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>trainers.tabular.train_gandalf<span class="w"> </span><span class="se">\</span>
--train_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--val_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--batch_size<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--num_epochs<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--learning_rate<span class="w"> </span>&lt;float&gt;<span class="w"> </span><span class="se">\</span>
--out<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--type<span class="w"> </span>&lt;classification_or_regression&gt;<span class="w"> </span><span class="se">\</span>
--categorical_cols<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--continuous_cols<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--tabular_target_column<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--gflu_stages<span class="w"> </span>&lt;int&gt;
</pre></div>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--categorical_cols</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Column names of the categorical fields to treat differently.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--continuous_cols</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Column names of the numeric fields.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--tabular_target_column</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Target column within the dataset.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--gflu_stages</span> <span class="pre">&lt;int&gt;</span></code></p></td>
<td><p>Number of GFLU stages for GANDALF.</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">--gflu_stages</span></code> is a hyperparameter related to internal GANDALF working, according to the documentation on <a class="reference external" href="https://pytorch-tabular.readthedocs.io/en/latest/apidocs_model/#pytorch_tabular.models.GANDALFConfig">PyTorch Tabular</a>, it is the number of layers in the feature abstraction layer. The documentation defaults to 6 and we advise the same.</p>
</div>
</section>
<section id="time-series">
<h2>Time Series<a class="headerlink" href="#time-series" title="Link to this heading"></a></h2>
<p>Currently, <strong>DeepTune</strong> offers only support for DeepAR model to provide predictions on your own time-series data. You can read more about DeepAR through the paper <a class="reference external" href="https://arxiv.org/abs/1704.04110">here</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DeepAR implementation (similar to GANDALF) does not support transfer learning in the way we commonly applied to images or text above. Instead, it follows the standard training scheme, starting from scratch.</p>
</div>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>trainers.timeseries.train_deepar<span class="w"> </span><span class="se">\</span>
--train_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--val_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--batch_size<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--num_epochs<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--out<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--time_idx_column<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--target<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--max_encoder_length<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--max_prediction_length<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--time_varying_known_categoricals<span class="w"> </span>&lt;list&gt;<span class="w"> </span><span class="se">\</span>
--time_varying_unknown_categoricals<span class="w"> </span>&lt;list&gt;<span class="w"> </span><span class="se">\</span>
--static_categoricals<span class="w"> </span>&lt;list&gt;<span class="w"> </span><span class="se">\</span>
--time_varying_known_reals<span class="w"> </span>&lt;list&gt;<span class="w"> </span><span class="se">\</span>
--time_varying_unknown_reals<span class="w"> </span>&lt;list&gt;<span class="w"> </span><span class="se">\</span>
--static_reals<span class="w"> </span>&lt;list&gt;<span class="w"> </span><span class="se">\</span>
--group_ids<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The columns related to categoricals, reals, and maximum prediction length are optional, you may skip them.</p>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--time_idx_column</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Name of the time index column in your dataset.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--target_column</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Name of the target column in your dataset.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--max_encoder_length</span> <span class="pre">&lt;int&gt;</span></code></p></td>
<td><p>Maximum encoder length for forecasting.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--max_prediction_length</span> <span class="pre">&lt;int&gt;</span></code></p></td>
<td><p>Maximum prediction length for forecasting.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--time_varying_known_categoricals</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Column names of time-varying known categorical features.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--time_varying_unknown_categoricals</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Column names of time-varying unknown categorical features.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--static_categoricals</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Column names of static categorical features.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--time_varying_known_reals</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Column names of time-varying known real-valued features.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--time_varying_unknown_reals</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Column names of time-varying unknown real-valued features.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">--static_reals</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Column names of static real-valued features.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">--group_ids</span> <span class="pre">&lt;str&gt;</span></code></p></td>
<td><p>Column names used to identify different time series groups. Set to default “0”.</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For more information about these arguments, you may refer to the <a class="reference external" href="https://pytorch-forecasting.readthedocs.io/en/v1.3.0/api/pytorch_forecasting.models.deepar._deepar.DeepAR.html">PyTorch Forecasting documentation</a>.</p>
</div>
</section>
<section id="tabpfn-support">
<h2>TabPFN Support<a class="headerlink" href="#tabpfn-support" title="Link to this heading"></a></h2>
<p>Currently, <strong>DeepTune</strong> offers support for TabPFN (Tabular Prior-data Fitted Network) model to get fine-tuned or trained from scratch on your own tabular data. You can read more about TabPFN through the paper <a class="reference external" href="https://arxiv.org/pdf/2511.08667">here</a>. We project to expand the support to include the time-series modality in the near future.</p>
<p>In order to run TabPFN in <strong>DeepTune</strong>, you can use the following CLI structure:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$ </span>python<span class="w"> </span>-m<span class="w"> </span>trainers.tabular.train_tabpfn<span class="w"> </span><span class="se">\</span>
--train_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--val_df<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--batch_size<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--num_epochs<span class="w"> </span>&lt;int&gt;<span class="w"> </span><span class="se">\</span>
--out<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--mode<span class="w"> </span>&lt;cls_or_reg&gt;<span class="w"> </span><span class="se">\</span>
--target_column<span class="w"> </span>&lt;str&gt;<span class="w"> </span><span class="se">\</span>
--<span class="o">[</span>finetuning-mode<span class="o">]</span><span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default, TabPFN will be used in training from scratch mode. To enable fine-tuning on your dataset, you need to add the <cite>–finetuning-mode</cite> switch to the previous command.</p>
</div>
</section>
<section id="training-output">
<h2>Training Output<a class="headerlink" href="#training-output" title="Link to this heading"></a></h2>
<p>After training completes, you may find the results in the directory specified with the <cite>–out</cite> directory. Alternatively, <strong>DeepTune</strong> will create an output directory named  <cite>deeptune_results</cite> (if it does not already exist). Inside this directory, the results are organized in a subfolder using the following naming convention: <code class="docutils literal notranslate"><span class="pre">trainval_output_&lt;FINETUNED/PEFT&gt;_&lt;model_version&gt;_&lt;mode&gt;_&lt;yyyymmdd_hhmm&gt;</span></code> with the following output:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">output_directory</span>
<span class="go">├── trainval_output_&lt;FINETUNED/PEFT&gt;_&lt;model_version&gt;_&lt;mode&gt;_&lt;yyyymmdd_hhmm&gt;</span>
<span class="go">    └── cli_arguments.json</span>
<span class="go">    └── model_weights.pth</span>
<span class="go">    └── training_log.csv</span>
<span class="go">    └── training_details.json</span>
</pre></div>
</div>
<table class="docutils align-default">
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">cli_arguments.json</span></code></p></td>
<td><p>Records the CLI arguments you entered to run <strong>DeepTune</strong>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">model_weights.pth</span></code></p></td>
<td><p>The fine-tuned model weights (used later for evaluation).</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">training_log.csv</span></code></p></td>
<td><p>A performance log reporting training and validation accuracies and errors for each epoch.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">training_details.json</span></code></p></td>
<td><p>Stores the amount of time needed between starting and completing the training.</p></td>
</tr>
</tbody>
</table>
<p>For text GPT-2 and BERT models, instead of the <code class="docutils literal notranslate"><span class="pre">model_weights.pth</span></code> file, you may find a whole subdirectory containing the tokenizer and model weights files. For TabPFN models, you may find the model weights stored in <cite>.tabpfn</cite> (if you are training from scratch) or <cite>.joblib</cite> (or if you are fine-tuning) format instead of <cite>.pth</cite>.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">output_directory</span>
<span class="go">└── trainval_output_&lt;BERT/GPT2&gt;_&lt;yyyymmdd_hhmm&gt;</span>
<span class="go">    ├── tokenizer</span>
<span class="go">    │   └── ...</span>
<span class="go">    ├── model</span>
<span class="go">    │   └── ...</span>
<span class="go">    ├── model_weights.pth</span>
<span class="go">    └── training_log.csv</span>
</pre></div>
</div>
<p>For tabular data using GANDALF, the directory will be named as <code class="docutils literal notranslate"><span class="pre">trainval_output_&lt;GANDALF&gt;_&lt;mode&gt;_&lt;yyyymmdd_hhmm&gt;</span></code> with the weights stored in <code class="docutils literal notranslate"><span class="pre">GANDALF_model</span></code> subdirectory. For the timeseries modality, the weights will be stored in <cite>.ckpt</cite> format instead of <cite>.pth</cite>, and <code class="docutils literal notranslate"><span class="pre">prediction_output.json</span></code> will be provided instead of <code class="docutils literal notranslate"><span class="pre">training_log.csv</span></code>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="split.html" class="btn btn-neutral float-left" title="Handling Datasets" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="evaluation.html" class="btn btn-neutral float-right" title="Using DeepTune for Evaluation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2026, Moayadeldin Hussain.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>